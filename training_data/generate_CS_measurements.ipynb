{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b0f3fc",
   "metadata": {},
   "source": [
    "# From raw measurements to compressed sensing measurements\n",
    "    Now we have generated raw FLIM measurements from generate_raw_measurement, we can use the raw measurements to simulate the compressively sensed measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sample_size = 8000\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d4b9e",
   "metadata": {},
   "source": [
    "## Some preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee50d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import draw\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6788c",
   "metadata": {},
   "source": [
    "A function to read in raw_measurement files into the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10068227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the raw measurements data from the Raw_Measurements file\n",
    "# Parameters:\n",
    "# - starting_index is where the index starts\n",
    "# - batch is the number of batches\n",
    "# For example, read_raw_data(20,10) will return the raw measurement data from sample_20 to sample_30\n",
    "# There are up to 8000 raw measurements data\n",
    "def read_raw_data(starting_index, batch):\n",
    "    raw_measurements = np.zeros([batch,128,128,256]).astype(np.uint8)\n",
    "    for j in range(batch):\n",
    "        i = j+starting_index\n",
    "        raw_measurements[j,:,:] = scipy.io.loadmat('Data\\Raw_Measurements\\sample_'+str(i)+'.mat')['measurements']\n",
    "    return raw_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c6b00",
   "metadata": {},
   "source": [
    "## Generate images\n",
    "Compressive measurements are made by $y = \\phi x$, where y is the compressively measured datasets, and x is the original image. $\\phi$ is the measurement matrix\n",
    "x in here is a 1D array, so we reshape the lifetime raw measurements into \n",
    "\n",
    "$x = [batch_i, t, y]$\n",
    "\n",
    "So first of all we need a Hadamard Matrix, as our measurement matrix $\\phi$\n",
    "### Generate hadamard patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_measurements = [batch_i, x, y, t]\n",
    "hadamard= scipy.linalg.hadamard(image_size*image_size)\n",
    "print('The shape of hadamard matrix is ', hadamard.shape)\n",
    "plt.imshow(hadamard[:128,:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe059326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hadamard order\n",
    "#with h5py.File('pattern_index_HR_128.mat','r') as f:\n",
    "#    hadamard_idx = np.squeeze((np.array(f['pat_idx'])-1).astype(int))\n",
    "# Reorder hadamard according to the order\n",
    "# Only keeping 1800 of hadamard matrix, compression of 10%\n",
    "#hadamard_ordered = hadamard[hadamard_idx[:1800],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb0d93",
   "metadata": {},
   "source": [
    "### Read in raw measurements\n",
    " We have previously generated 8000 raw measurements samples, so there's where the 8000 comes from. \n",
    " Future sample data might be bigger or smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fe71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch size of multiples of 8000 to use all samples\n",
    "def batch_measurements(batch_size):\n",
    "    total_batch = sample_size//batch_size\n",
    "    for batch in range(total_batch):\n",
    "        # read 1 data per batch, starting from 0\n",
    "        raw_measurements = read_raw_data(batch*batch_size ,batch_size)\n",
    "        # Raw data is in format of [batch_i,x,y,t]\n",
    "        # we need [batch_i,t,pixel] to Apply hadamard matrix\n",
    "        x_2D = np.moveaxis(raw_measurements,[1,2,3],[2,3,1])\n",
    "        # x_2D = [batch_i,t,x,y]\n",
    "        x = np.reshape(x_2D, (x_2D.shape[0],x_2D.shape[1],-1))\n",
    "        # x = [batch_i,t,pixel_i]\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08fd33",
   "metadata": {},
   "source": [
    "## Apply hadamard pattern to the images\n",
    "CPU based computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate some memories \n",
    "cs_data = np.zeros((sample_size,256,image_size*image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "for i, x in enumerate(batch_measurements(batch_size)):\n",
    "    #cs_data[i*batch_size:(i+1)*batch_size,:,:] = np.dot(x,hadamard_ordered.T)\n",
    "    cs_data[i*batch_size:(i+1)*batch_size,:,:] = np.dot(x,hadamard.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b60ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_measurements.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f510cce",
   "metadata": {},
   "source": [
    " Alternatively, GPU vectorised based computation based on cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate some memories \n",
    "cs_data = np.zeros((sample_size,256,image_size*image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "for i, x in enumerate(tqdm.tqdm(batch_measurements(batch_size))):\n",
    "    # load into gpu memory\n",
    "    x_gpu = cp.asarray(x)\n",
    "    hadamard_gpu = cp.asarray(hadamard_ordered.T)\n",
    "\n",
    "    # matrix dot product\n",
    "    CS_measurements_gpu = cp.dot(x_gpu, hadamard_gpu)\n",
    "\n",
    "    # Move back to host memory\n",
    "    cs_data[i*batch_size : (i+1)*batch_size] = cp.asnumpy(CS_measurements_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into files\n",
    "with h5py.File('Data/cs_measurements.h5','w') as f:\n",
    "    f.create_dataset('cs_data', data=cs_data)\n",
    "# draw(characters[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630d210",
   "metadata": {},
   "source": [
    "# Writing into .mat\n",
    "Now we have all the data, we just need to make all files into the format that's required by the CNN\n",
    "The format takes as sample_int.mat, the mat is as \n",
    "{'cs_data': [], 'intensity_image':[], 'lifetime_image': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9ea316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(idx):\n",
    "    f_gt = h5py.File('Data/Ground_Truth/ground_truth.h5','r')\n",
    "    f_cs = h5py.File('Data/CS_Measurements/cs_measurements.h5','r')\n",
    "    intensity_image = np.array(f_gt['intensity_ground_truth'][idx])\n",
    "    lifetime_image = np.array(f_gt['lifetime_ground_truth'][idx])\n",
    "    cs_data = np.array(f_cs['cs_data'][idx])\n",
    "    f_gt.close()\n",
    "    f_cs.close()\n",
    "    return cs_data, intensity_image, lifetime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mat_fromfile(idx):\n",
    "    cs_data, intensity_image, lifetime_image = get_data(idx)\n",
    "    scipy.io.savemat(('Data/training_data/sample_'+str(idx)+'.mat'), {'cs_data': cs_data, 'intensity_image': intensity_image, 'lifetime_image': lifetime_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c006c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8000/8000 [06:03<00:00, 22.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(sample_size)):\n",
    "    write_mat(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61804832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
